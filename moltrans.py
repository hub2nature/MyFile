# -*- coding: utf-8 -*-
"""MolTrans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LnWLkl_eDHmYqO9BtKjsa3v3oM9JQg2U
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/DTI_prediction/MolTrans/

# Commented out IPython magic to ensure Python compatibility.
# %cd MolTrans

!pip install torch

!pip install pandas

import os
import torch
import random
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

def set_seed(seed_value=42):
        os.environ['PYTHONHASHSEED'] = str(seed_value)
        random.seed(seed_value)
        np.random.seed(seed_value)
        torch.manual_seed(seed_value)
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False




set_seed(42)

"""# **DAVIS data on GCVAE**"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import math

# Load the dataset
file_path = '/content/drive/MyDrive/DTI_prediction/MolTrans/MolTrans/dataset/DAVIS/train.csv'
df = pd.read_csv(file_path)
df = df.sample(n=100, random_state=42).reset_index(drop=True)
# Assuming the columns are named 'SMILES', 'Target Sequence', and 'Label'
df = df[['SMILES', 'Target Sequence', 'Label']]
print(len(df))
print(df)
df_val=pd.read_csv('/content/drive/MyDrive/DTI_prediction/MolTrans/MolTrans/dataset/DAVIS/val.csv')
df_val = df_val.sample(n=10, random_state=42).reset_index(drop=True)
# Assuming the columns are named 'SMILES', 'Target Sequence', and 'Label'
df_val = df_val[['SMILES', 'Target Sequence', 'Label']]
print(len(df_val))
print(df_val)


df_test=pd.read_csv('/content/drive/MyDrive/DTI_prediction/MolTrans/MolTrans/dataset/DAVIS/test.csv')
df_test = df_test.sample(n=10, random_state=42).reset_index(drop=True)
# Assuming the columns are named 'SMILES', 'Target Sequence', and 'Label'
df_test = df_test[['SMILES', 'Target Sequence', 'Label']]
print(len(df_test))
print(df_test)

2
                                              SMILES  \
0  CCN(CC)CCNC(=O)C1=C(NC(=C1C)C=C2C3=C(C=CC(=C3)...
1  CC12C(C(CC(O1)N3C4=CC=CC=C4C5=C6C(=C7C8=CC=CC=...

                                     Target Sequence  Label
0  MSGTSKESLGHGGLPGLGKTCLTTMDTKLNMLNEKVDQLLHFQEDV...      1
1  MPARIGYYEIDRTIGKGNFAVVKRATHLVTKAKVAIKIIDKTQLDE...      1
1
                                              SMILES  \
0  CNC(=O)C1=NC=CC(=C1)OC2=CC=C(C=C2)NC(=O)NC3=CC...

                                     Target Sequence  Label
0  MSSMPKPERHAESLLDICHDTNSSPTDLMTVTKNQNIILQSISRSE...      0
1
                                    SMILES  \
0  CN1CCN(CC1)C(=O)C2=CC3=C(N2)C=CC(=C3)Cl

                                     Target Sequence  Label
0  MARTTSQLYDAVPIQSSVVLCSCPSPSMVRTQTESSTPPGIPGGSR...      0



run 2
                                     2
                                              SMILES  \
0  CCN(CC)CCNC(=O)C1=C(NC(=C1C)C=C2C3=C(C=CC(=C3)...
1  CC12C(C(CC(O1)N3C4=CC=CC=C4C5=C6C(=C7C8=CC=CC=...

                                     Target Sequence  Label
0  MSGTSKESLGHGGLPGLGKTCLTTMDTKLNMLNEKVDQLLHFQEDV...      1
1  MPARIGYYEIDRTIGKGNFAVVKRATHLVTKAKVAIKIIDKTQLDE...      1
1
                                              SMILES  \
0  CNC(=O)C1=NC=CC(=C1)OC2=CC=C(C=C2)NC(=O)NC3=CC...

                                     Target Sequence  Label
0  MSSMPKPERHAESLLDICHDTNSSPTDLMTVTKNQNIILQSISRSE...      0
1
                                    SMILES  \
0  CN1CCN(CC1)C(=O)C2=CC3=C(N2)C=CC(=C3)Cl

                                     Target Sequence  Label
0  MARTTSQLYDAVPIQSSVVLCSCPSPSMVRTQTESSTPPGIPGGSR...      0

# Tokenization: Extract unique characters from SMILES and protein sequences
smiles_strings = df['SMILES'].tolist()
protein_sequences = df['Target Sequence'].tolist()

smiles_strings_val = df_val['SMILES'].tolist()
protein_sequences_val = df_val['Target Sequence'].tolist()

smiles_strings_test = df_test['SMILES'].tolist()
protein_sequences_test = df_test['Target Sequence'].tolist()

# Combine all tokens from training, validation, and test sets
all_smiles_tokens = set(''.join(smiles_strings + smiles_strings_val + smiles_strings_test))
all_protein_tokens = set(''.join(protein_sequences + protein_sequences_val + protein_sequences_test))
#all_tokens = list(all_smiles_tokens.union(all_protein_tokens))
all_tokens = sorted(all_smiles_tokens.union(all_protein_tokens))

# Convert tokens to numerical representations
token_to_idx = {token: idx for idx, token in enumerate(all_tokens)}


def tokenize_and_pad(sequences, token_to_idx, max_len):
    token_sequences = [[token_to_idx[token] for token in seq] for seq in sequences]
    padded_sequences = [seq + [0] * (max_len - len(seq)) for seq in token_sequences]
    return torch.tensor(padded_sequences, dtype=torch.long).to(device)

# Find max lengths
max_smiles_len = max(max(len(seq) for seq in smiles_strings),
                     max(len(seq) for seq in smiles_strings_val),
                     max(len(seq) for seq in smiles_strings_test))
max_protein_len = max(max(len(seq) for seq in protein_sequences),
                      max(len(seq) for seq in protein_sequences_val),
                      max(len(seq) for seq in protein_sequences_test))

print('all_smiles_tokens',all_smiles_tokens)
print('all_protein_tokens',all_protein_tokens)
print('all_tokens',all_tokens)

#run 1
all_smiles_tokens1= {'7', '(', 'O', '=', 'C', '1', 'N', 'F', '4', '6', 'l', ')', '2', '8', '5', '3'}
all_protein_tokens1 ={'V', 'C', 'N', 'F', 'K', 'A', 'H', 'L', 'Y', 'T', 'G', 'W', 'M', 'D', 'S', 'Q', 'P', 'E', 'R', 'I'}
all_tokens1 =['(', ')', '1', '2', '3', '4', '5', '6', '7', '8', '=', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'l']


#run 2
all_smiles_tokens2= {')', 'F', '6', 'N', 'C', 'O', 'l', '2', '7', '1', '3', '4', '8', '=', '5', '('}
all_protein_tokens2 ={'R', 'N', 'L', 'K', 'Q', 'S', 'H', 'I', 'A', 'V', 'P', 'E', 'F', 'W', 'T', 'Y', 'G', 'M', 'C', 'D'}
all_tokens2= ['(', ')', '1', '2', '3', '4', '5', '6', '7', '8', '=', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'l']

print(all_smiles_tokens1==all_smiles_tokens2)
print(all_protein_tokens1==all_protein_tokens2)
print(set(all_tokens1)==set(all_tokens2))

print('token_to_idx ',token_to_idx)

#run 1
token_to_idx1=  {'(': 0, ')': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '=': 10, 'A': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'K': 19, 'L': 20, 'M': 21, 'N': 22, 'O': 23, 'P': 24, 'Q': 25, 'R': 26, 'S': 27, 'T': 28, 'V': 29, 'W': 30, 'Y': 31, 'l': 32}

#run 1
token_to_idx2=  {'(': 0, ')': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '=': 10, 'A': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'K': 19, 'L': 20, 'M': 21, 'N': 22, 'O': 23, 'P': 24, 'Q': 25, 'R': 26, 'S': 27, 'T': 28, 'V': 29, 'W': 30, 'Y': 31, 'l': 32}
#token_to_idx2=  {'(': 0, ')': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '=': 10, 'A': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'K': 19, 'L': 20, 'M': 21, 'N': 22, 'O': 23, 'P': 24, 'Q': 25, 'R': 26, 'S': 27, 'T': 28, 'V': 29, 'W': 30, 'Y': 31, 'l': 32}
print(token_to_idx1==token_to_idx2)

# Tokenize and pad sequences
set_seed(42)
smiles_train = tokenize_and_pad(smiles_strings, token_to_idx, max_smiles_len)
protein_train = tokenize_and_pad(protein_sequences, token_to_idx, max_protein_len)
labels_train = torch.tensor(df['Label'].values, dtype=torch.float32).view(-1, 1).to(device)

print('smiles_train',smiles_train)
print('protein_train',protein_train)
print('labels_train',labels_train)

smiles_val = tokenize_and_pad(smiles_strings_val, token_to_idx, max_smiles_len)
protein_val = tokenize_and_pad(protein_sequences_val, token_to_idx, max_protein_len)
labels_val = torch.tensor(df_val['Label'].values, dtype=torch.float32).view(-1, 1).to(device)

smiles_test = tokenize_and_pad(smiles_strings_test, token_to_idx, max_smiles_len)
protein_test = tokenize_and_pad(protein_sequences_test, token_to_idx, max_protein_len)
labels_test = torch.tensor(df_test['Label'].values, dtype=torch.float32).view(-1, 1).to(device)

run 1
smiles_train tensor([[12, 12, 22,  0, 12, 12,  1, 12, 12, 22, 12,  0, 10, 23,  1, 12,  2, 10,
         12,  0, 22, 12,  0, 10, 12,  2, 12,  1, 12, 10, 12,  3, 12,  4, 10, 12,
          0, 12, 10, 12, 12,  0, 10, 12,  4,  1, 15,  1, 22, 12,  3, 10, 23,  1,
         12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
        [12, 12,  2,  3, 12,  0, 12,  0, 12, 12,  0, 23,  2,  1, 22,  4, 12,  5,
         10, 12, 12, 10, 12, 12, 10, 12,  5, 12,  6, 10, 12,  7, 12,  0, 10, 12,
          8, 12,  9, 10, 12, 12, 10, 12, 12, 10, 12,  9, 22,  3, 12,  8, 10, 12,
          6,  4,  1, 12, 22, 12,  7, 10, 23,  1, 22, 12,  1, 23, 12]])
protein_train tensor([[21, 27, 16,  ...,  0,  0,  0],
        [21, 24, 11,  ...,  0,  0,  0]])
labels_train tensor([[1.],
        [1.]])

run 2
smiles_train tensor([[12, 12, 22,  0, 12, 12,  1, 12, 12, 22, 12,  0, 10, 23,  1, 12,  2, 10,
         12,  0, 22, 12,  0, 10, 12,  2, 12,  1, 12, 10, 12,  3, 12,  4, 10, 12,
          0, 12, 10, 12, 12,  0, 10, 12,  4,  1, 15,  1, 22, 12,  3, 10, 23,  1,
         12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],
        [12, 12,  2,  3, 12,  0, 12,  0, 12, 12,  0, 23,  2,  1, 22,  4, 12,  5,
         10, 12, 12, 10, 12, 12, 10, 12,  5, 12,  6, 10, 12,  7, 12,  0, 10, 12,
          8, 12,  9, 10, 12, 12, 10, 12, 12, 10, 12,  9, 22,  3, 12,  8, 10, 12,
          6,  4,  1, 12, 22, 12,  7, 10, 23,  1, 22, 12,  1, 23, 12]])
protein_train tensor([[21, 27, 16,  ...,  0,  0,  0],
        [21, 24, 11,  ...,  0,  0,  0]])
labels_train tensor([[1.],
        [1.]])

from torch.utils.data import Dataset

class DTI_Dataset(Dataset):
    def __init__(self, smiles, proteins, labels):
        self.smiles = smiles
        self.proteins = proteins
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.smiles[idx], self.proteins[idx], self.labels[idx]

from torch.utils.data import DataLoader

batch_size = 5

train_dataset = DTI_Dataset(smiles_train, protein_train, labels_train)
val_dataset = DTI_Dataset(smiles_val, protein_val, labels_val)
test_dataset = DTI_Dataset(smiles_test, protein_test, labels_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define the Transformer model for SMILES sequences
class SMILESTransformer(nn.Module):
    def __init__(self, num_tokens, embed_dim, num_heads, num_encoder_layers, dropout=0.5):
        super(SMILESTransformer, self).__init__()
        self.embedding = nn.Embedding(num_tokens, embed_dim)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True),
            num_layers=num_encoder_layers
        )
        self.fc = nn.Linear(embed_dim, embed_dim)

    def forward(self, smiles):
        smiles_emb = self.embedding(smiles)  # Transformer with batch_first=True expects input as (batch_size, seq_len, embed_dim)
        smiles_encoded = self.encoder(smiles_emb).mean(dim=1)  # Global average pooling
        return self.fc(smiles_encoded)

# Define the Transformer model for Protein sequences
class ProteinTransformer(nn.Module):
    def __init__(self, num_tokens, embed_dim, num_heads, num_encoder_layers, dropout=0.5):
        super(ProteinTransformer, self).__init__()
        self.embedding = nn.Embedding(num_tokens, embed_dim)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True),
            num_layers=num_encoder_layers
        )
        self.fc = nn.Linear(embed_dim, embed_dim)

    def forward(self, proteins):
        proteins_emb = self.embedding(proteins)  # Transformer with batch_first=True expects input as (batch_size, seq_len, embed_dim)
        proteins_encoded = self.encoder(proteins_emb).mean(dim=1)  # Global average pooling
        return self.fc(proteins_encoded)

class DTIModel(nn.Module):
    def __init__(self, smiles_transformer, protein_transformer, embed_dim):
        super(DTIModel, self).__init__()
        self.smiles_transformer = smiles_transformer
        self.protein_transformer = protein_transformer
        self.fc = nn.Linear(embed_dim * 2, 1)

    def forward(self, smiles, proteins):
        smiles_out = self.smiles_transformer(smiles)
        proteins_out = self.protein_transformer(proteins)
        combined = torch.cat((smiles_out, proteins_out), dim=1)
        return self.fc(combined)

# Model Initialization
from sklearn.metrics import roc_auc_score, roc_curve
from torch.optim.lr_scheduler import StepLR
num_tokens = len(token_to_idx)
embed_dim = 4
num_heads = 1
num_encoder_layers = 1


set_seed(42)
smiles_transformer = SMILESTransformer(num_tokens, embed_dim, num_heads, num_encoder_layers)
protein_transformer = ProteinTransformer(num_tokens, embed_dim, num_heads, num_encoder_layers)
dti_model = DTIModel(smiles_transformer, protein_transformer, embed_dim).to(device)

# Debugging: Check model parameters
print(dti_model)

# Training loop
optimizer = optim.Adam(dti_model.parameters(), lr=0.01)
criterion = nn.MSELoss()
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

def train_model(model, train_loader, val_loader, epochs=2):
    model.train()
    loss_values = []
    val_loss_values = []

    for epoch in range(epochs):
        total_loss = 0
        for smiles_batch, protein_batch, labels_batch in train_loader:
            smiles_batch = smiles_batch.to(device)
            protein_batch = protein_batch.to(device)
            labels_batch = labels_batch.to(device)
            # print('smiles_batch',smiles_batch)
            # print('protein_batch',protein_batch)
            # print('labels_batch',labels_batch)

            optimizer.zero_grad()
            outputs = model(smiles_batch, protein_batch)
            loss = criterion(outputs, labels_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        loss_values.append(avg_loss)

        # Validation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for smiles_batch, protein_batch, labels_batch in val_loader:
                smiles_batch = smiles_batch.to(device)
                protein_batch = protein_batch.to(device)
                labels_batch = labels_batch.to(device)

                val_outputs = model(smiles_batch, protein_batch)
                val_loss = criterion(val_outputs, labels_batch)
                total_val_loss += val_loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        val_loss_values.append(avg_val_loss)
        scheduler.step()

        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
    return loss_values, val_loss_values

# Run the training
losses, val_losses = train_model(dti_model, train_loader, val_loader, epochs=5)

# Plotting the training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(losses,  label='Training Loss')
plt.plot(val_losses,  label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Testing loop with AUC and ROC curve 5704
def test_model(model, test_loader):
    model.eval()
    all_labels = []
    all_outputs = []
    with torch.no_grad():
        for smiles_batch, protein_batch, labels_batch in test_loader:
            smiles_batch = smiles_batch.to(device)
            protein_batch = protein_batch.to(device)
            labels_batch = labels_batch.to(device)

            outputs = model(smiles_batch, protein_batch)
            all_outputs.append(outputs)
            all_labels.append(labels_batch)

    all_outputs = torch.cat(all_outputs).cpu().numpy()
    all_labels = torch.cat(all_labels).cpu().numpy()

    auc = roc_auc_score(all_labels, all_outputs)
    fpr, tpr, _ = roc_curve(all_labels, all_outputs)

    print(f'Test AUC: {auc:.4f}')

    plt.figure(figsize=(10, 5))
    plt.plot(fpr, tpr)
    plt.title('ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.grid(True)
    plt.show()

# Run the testing
test_model(dti_model, test_loader)

run 1
Epoch 1/5, Loss: 0.2683, Val Loss: 0.1306
Epoch 2/5, Loss: 0.2652, Val Loss: 0.1533
Epoch 3/5, Loss: 0.2721, Val Loss: 0.2481
Epoch 4/5, Loss: 0.2526, Val Loss: 0.2370
Epoch 5/5, Loss: 0.2485, Val Loss: 0.2502

Test AUC: 0.5556

run 2
Epoch 1/5, Loss: 0.2683, Val Loss: 0.1306
Epoch 2/5, Loss: 0.2652, Val Loss: 0.1533
Epoch 3/5, Loss: 0.2721, Val Loss: 0.2481
Epoch 4/5, Loss: 0.2526, Val Loss: 0.2370
Epoch 5/5, Loss: 0.2485, Val Loss: 0.2502


Test AUC: 0.5556


run 3
Epoch 1/5, Loss: 0.2683, Val Loss: 0.1306
Epoch 2/5, Loss: 0.2652, Val Loss: 0.1533
Epoch 3/5, Loss: 0.2721, Val Loss: 0.2481
Epoch 4/5, Loss: 0.2526, Val Loss: 0.2370
Epoch 5/5, Loss: 0.2485, Val Loss: 0.2502

Test AUC: 0.5556

dict1={'#': 0, '(': 1, ')': 2, '.': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '=': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'V': 33, 'W': 34, 'X': 35, 'Y': 36, 'l': 37, 'r': 38}
dict2={'#': 0, '(': 1, ')': 2, '.': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '=': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'V': 33, 'W': 34, 'X': 35, 'Y': 36, 'l': 37, 'r': 38}

print(dict1==dict2)

Run 1

Epoch 1/3, Loss: 0.2655, Val Loss: 0.1467
Epoch 2/3, Loss: 0.2648, Val Loss: 0.1667
Epoch 3/3, Loss: 0.2713, Val Loss: 0.2460

Test AUC: 0.6667
run 2

Epoch 1/3, Loss: 0.2691, Val Loss: 0.1461
Epoch 2/3, Loss: 0.2650, Val Loss: 0.1645
Epoch 3/3, Loss: 0.2715, Val Loss: 0.2478
Test AUC: 0.6389


run 3
Epoch 1/3, Loss: 0.2669, Val Loss: 0.1470
Epoch 2/3, Loss: 0.2644, Val Loss: 0.1585
Epoch 3/3, Loss: 0.2739, Val Loss: 0.2438

Test AUC: 0.5278

run 4
Epoch 1/3, Loss: 0.2642, Val Loss: 0.1498
Epoch 2/3, Loss: 0.2635, Val Loss: 0.1549
Epoch 3/3, Loss: 0.2745, Val Loss: 0.2434

Test AUC: 0.6944

run 1
smiles_batch tensor([[12, 12, 22,  0, 12, 12,  1, 12, 12, 22, 12,  0, 10, 23,  1, 12,  2, 10,
         12,  0, 22, 12,  0, 10, 12,  2, 12,  1, 12, 10, 12,  3, 12,  4, 10, 12,
          0, 12, 10, 12, 12,  0, 10, 12,  4,  1, 15,  1, 22, 12,  3, 10, 23,  1,
         12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])
protein_batch tensor([[21, 27, 16,  ...,  0,  0,  0]])
labels_batch tensor([[1.]])
smiles_batch tensor([[12, 12,  2,  3, 12,  0, 12,  0, 12, 12,  0, 23,  2,  1, 22,  4, 12,  5,
         10, 12, 12, 10, 12, 12, 10, 12,  5, 12,  6, 10, 12,  7, 12,  0, 10, 12,
          8, 12,  9, 10, 12, 12, 10, 12, 12, 10, 12,  9, 22,  3, 12,  8, 10, 12,
          6,  4,  1, 12, 22, 12,  7, 10, 23,  1, 22, 12,  1, 23, 12]])
protein_batch tensor([[21, 24, 11,  ...,  0,  0,  0]])
labels_batch tensor([[1.]])
Epoch 1/1, Loss: 0.4964, Val Loss: 0.9884


run 2
smiles_batch tensor([[12, 12, 22,  0, 12, 12,  1, 12, 12, 22, 12,  0, 10, 23,  1, 12,  2, 10,
         12,  0, 22, 12,  0, 10, 12,  2, 12,  1, 12, 10, 12,  3, 12,  4, 10, 12,
          0, 12, 10, 12, 12,  0, 10, 12,  4,  1, 15,  1, 22, 12,  3, 10, 23,  1,
         12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])
protein_batch tensor([[21, 27, 16,  ...,  0,  0,  0]])
labels_batch tensor([[1.]])
smiles_batch tensor([[12, 12,  2,  3, 12,  0, 12,  0, 12, 12,  0, 23,  2,  1, 22,  4, 12,  5,
         10, 12, 12, 10, 12, 12, 10, 12,  5, 12,  6, 10, 12,  7, 12,  0, 10, 12,
          8, 12,  9, 10, 12, 12, 10, 12, 12, 10, 12,  9, 22,  3, 12,  8, 10, 12,
          6,  4,  1, 12, 22, 12,  7, 10, 23,  1, 22, 12,  1, 23, 12]])
protein_batch tensor([[21, 24, 11,  ...,  0,  0,  0]])
labels_batch tensor([[1.]])
Epoch 1/1, Loss: 0.4964, Val Loss: 0.9884

import matplotlib.pyplot as plt

# Epochs and their corresponding losses
epochs = list(range(1, 76))
train_loss = [
    0.027885513563207412, 0.054137402445502476, 0.06470613319157531, 0.056876578947928365,
    0.04976940533498172, 0.050372314755796024, 0.05223546346380709, 0.05635556905235269,
    0.05810962719605961, 0.055057774578908696, 0.17281309154824323, 0.3013385314195516,
    0.32241009245448893, 0.3103095854113575, 0.30309196988134895, 0.2978185044445154,
    0.2940276807159868, 0.2910408377078653, 0.2885978807702319, 0.2865333615709807,
    0.27885453141372624, 0.27450946075770694, 0.2707791896039293, 0.26764565933751694,
    0.2650463750571695, 0.26291583990322726, 0.26118857919714833, 0.2598015507896438,
    0.25869683034092417, 0.257822478886779, 0.25376512314982086, 0.25367437610189425,
    0.2535849548023166, 0.2534974525675519, 0.25341191046110545, 0.2533283299617185,
    0.2532466251886528, 0.25316688907510454, 0.2530890027530321, 0.25301297020366176,
    0.2525604728979009, 0.25255278605996195, 0.2525451128719417, 0.252537400326656,
    0.2525296720839639, 0.2525219333763341, 0.25251417692381006, 0.25250643184621824,
    0.25249868483488797, 0.25249091712118105, 0.25244389281018087, 0.2524431507096036,
    0.2524424057652932, 0.25244166138972945, 0.252440913601686, 0.2524401646761494,
    0.25243941484061816, 0.2524386677350707, 0.2524379172170435, 0.25243716294528873,
    0.25243261422364766, 0.25243254972778206, 0.2524324818194367, 0.2524324115223557,
    0.25243234839148193, 0.25243228309937105, 0.2524322158735217, 0.25243215115015744,
    0.2524320844930547, 0.2524320172672053, 0.2524314477243496, 0.25243144533561385,
    0.25243144317437677, 0.2524314520468239, 0.25243145932678046
]
val_loss = [
    0.057672729238897484, 0.05291033721721063, 0.05274562339713776, 0.05339497068114076,
    0.0538364814240285, 0.05336937963203463, 0.05242985166760844, 0.0534634834837663,
    0.052491300324695754, 0.05315238107250371, 0.050447157342630895, 0.056239553964994055,
    0.07009148615551122, 0.08281962472469882, 0.09373571204536772, 0.10277925320762268,
    0.11049481453572182, 0.1171540470516428, 0.12291214694368079, 0.12776021746561883,
    0.13617636762717936, 0.14475506829454543, 0.15315784030455223, 0.16126948991354476,
    0.1689883569612148, 0.1762318438354959, 0.1829393555984852, 0.18907303933767564,
    0.19461717353539265, 0.1995750627619155, 0.20020416252156523, 0.2008361535820555,
    0.20146478340029716, 0.20208962848211856, 0.2027106155898977, 0.20332762812997432,
    0.20394039907036943, 0.20454894862276443, 0.2051530488310976, 0.20575255306160195,
    0.205814123391471, 0.20587591906177236, 0.2059379517239459, 0.20600014092757346,
    0.20606242952511666, 0.20612486602461083, 0.2061873531722008, 0.20624991419150474,
    0.20631257381210935, 0.2063752850319477, 0.2063812734757332, 0.20638727800960235,
    0.20639329142075905, 0.206399327579965, 0.20640534717351833, 0.20641138396681624,
    0.20641742186977508, 0.2064234792710619, 0.2064295560914151, 0.2064356226870354,
    0.2064361515989963, 0.20643668605926188, 0.2064372238485103, 0.20643774990705735,
    0.20643827469742043, 0.20643880305455087, 0.20643935519013, 0.20643987887083215,
    0.2064404014418734, 0.20644094866323978, 0.2064409520714841, 0.2064409726002115,
    0.20644097925817712, 0.2064410073167466, 0.2064410149258502
]

# Plotting the training and validation loss over epochs
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Data for epochs and losses
epochs = list(range(1, 76))
train_losses = [0.027885515846372364, 0.05413740716972967, 0.06470594577238478, 0.056872749525476676,
                0.04978259596775753, 0.05036234763402924, 0.05225398897118551, 0.05636104986904837,
                0.05816884351913848, 0.05499937415173373, 0.1719733663432126, 0.3006366321890689,
                0.322451566694347, 0.31050923665743746, 0.3032364703657973, 0.2978730151102743,
                0.29403996655276715, 0.29104728735130253, 0.28860182792858313, 0.28651882150700986,
                0.27876255018565493, 0.2744487252171713, 0.27074124032759483, 0.2676237848878817,
                0.2650352815407833, 0.26291166086688295, 0.2611884523666542, 0.25980378368883644,
                0.2587001393087038, 0.25782626264422903, 0.2537752484319774, 0.2536844069721135,
                0.25359486077578014, 0.25350734511859546, 0.25342172782384714, 0.2533380578037437,
                0.25325628341609285, 0.25317642684201247, 0.25309838240838234, 0.2530222416834067,
                0.2525697353914494, 0.252562043434791, 0.2525543337332383, 0.25254659388811534,
                0.25253884323680675, 0.25253108746677866, 0.252523317364336, 0.2525155504468743,
                0.2525077636232813, 0.2524999671309959, 0.25245286217172636, 0.25245211802366124,
                0.25245136932562323, 0.2524506217650785, 0.2524498721570459, 0.2524491198190296,
                0.2524483696422504, 0.25244761514299696, 0.25244685893750374, 0.25244609852328553,
                0.25244153945045616, 0.25244147484084123, 0.25244140352001626, 0.2524413373179108,
                0.25244126952331486, 0.25244120263871345, 0.2524411340478722, 0.25244106670827354,
                0.2524409981174323, 0.2524409325978228, 0.2524403624862205, 0.2524403618037246,
                0.2524403566850051, 0.25244035418252, 0.2524403481538059]
val_losses = [0.057672734313490896, 0.05291034716507985, 0.05274878637245375, 0.05339748992238803,
              0.0538370836917006, 0.053368993714823405, 0.05242837837485762, 0.05347260621278487,
              0.05247709573906948, 0.05310134146350265, 0.050447293097757674, 0.05611275319763003,
              0.06985736499599954, 0.08264749675513582, 0.09366227341617675, 0.10275233116872767,
              0.11047215886572574, 0.11715261650053745, 0.12297402171695486, 0.1279156896820728,
              0.1362893051527282, 0.14482787742893746, 0.15319484424717883, 0.16127580776810646,
              0.16896953814207238, 0.17619335841625294, 0.1828862523778956, 0.1890100266863691,
              0.19454782773205576, 0.19950237426351994, 0.20013110078078636, 0.2007628213217918,
              0.20139099197818877, 0.20201563018750637, 0.2026365192488153, 0.20325358703415444,
              0.20386658046156803, 0.20447542581786501, 0.20507986788102922, 0.20567985362512,
              0.20574149211987536, 0.20580339534802639, 0.2058654875355832, 0.20592775433621507,
              0.20599015041234645, 0.2060526798855751, 0.20611530692970498, 0.2061780570669377,
              0.20624088297816032, 0.20630375343434354, 0.2063097580474742, 0.20631577692767408,
              0.206321813403926, 0.2063278559833131, 0.20633389872122318, 0.20633995992706178,
              0.2063460275530815, 0.20635212109761036, 0.2063582071915586, 0.20636430057756444,
              0.20636483678158293, 0.20636538027765902, 0.20636592123736727, 0.20636646124593755,
              0.20636700466275215, 0.20636755053667313, 0.2063680903867204, 0.20636863324870455,
              0.2063691759521657, 0.20636971580221297, 0.20636973458718746, 0.20636974457413593,
              0.20636976074348104, 0.2063697815099929, 0.20636979419183224]

# Plotting the data
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_losses, label='Training Loss', color='blue')
plt.plot(epochs, val_losses, label='Validation Loss', color='red')
plt.title('Training and Validation Loss over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Epoch data
epochs = range(1, 76)

# Training losses
train_losses = [
    0.02788551456612319, 0.054137400281555045, 0.06470558744638766, 0.05687185047768251,
    0.04971402312187348, 0.0503881803791704, 0.052194923853879485, 0.05636368044853685,
    0.05815917617670038, 0.05494987044462671, 0.1729668497001113, 0.3009969307255677,
    0.3222152545678479, 0.3104413881158556, 0.3032537900650774, 0.2979647842527346,
    0.294160783973359, 0.2911838956802856, 0.2887395994581339, 0.28665747322881496,
    0.2790352170362727, 0.2746624662220933, 0.27090748098060374, 0.2677520818382729,
    0.26513377537254157, 0.26298729665861786, 0.26124714849559405, 0.2598502827054672,
    0.25873818868444165, 0.257858598960265, 0.2537824840263556, 0.25369100193030963,
    0.25360079325792445, 0.25351261944716214, 0.2534263579900028, 0.25334205415867667,
    0.2532596827008342, 0.2531793259709846, 0.2531007461192954, 0.25302410455605456,
    0.2525686171218639, 0.2525608777317382, 0.25255311286176435, 0.25254531739322283,
    0.2525375193084469, 0.25252972167866833, 0.2525219061902461, 0.2525140843318619,
    0.2525062517810414, 0.2524984134290055, 0.2524510255751719, 0.25245027608088866,
    0.25244952374287233, 0.2524487729973465, 0.2524480190668397, 0.2524472635438424,
    0.25244650381212014, 0.25244574430789657, 0.2524449782062123, 0.2524442197257326,
    0.2524396231156269, 0.2524395607809984, 0.2524394927589038, 0.2524394247368092,
    0.252439355463472, 0.25243928971636387, 0.25243922567549554, 0.2524391600421367,
    0.2524390924750394, 0.2524390242254461, 0.25243844979136953, 0.2524384536588465,
    0.25243845092886275, 0.25243845081511346, 0.2524384535450972
]

# Validation losses
val_losses = [
    0.057672730351035345, 0.05291033864459001, 0.05274867951348302, 0.05339725246093297,
    0.05383292817382523, 0.05335058434071982, 0.05243085065620185, 0.05346989200236923,
    0.05253104145138306, 0.05313184938663266, 0.05044705761680776, 0.05614979830669596,
    0.06978301658354541, 0.08244540011312099, 0.09334026724892727, 0.10236632915094812,
    0.11005146595391821, 0.1167143795718538, 0.12251597963907618, 0.12743293779327514,
    0.1358558557372778, 0.14444580230307072, 0.1528661034684232, 0.16100126251261285,
    0.16874801811385662, 0.17602160469965733, 0.1827593949881006, 0.18892188076960278,
    0.19449171717179584, 0.19947133934561243, 0.20010436746351262, 0.2007405475733128,
    0.20137316979309347, 0.2020022376103604, 0.20262748914513182, 0.20324871134250722,
    0.20386576351333172, 0.20447868520909168, 0.2050870601484116, 0.2056908483834977,
    0.2057528981661543, 0.20581523392428744, 0.20587773953980587, 0.20594037625383824,
    0.20600317307609192, 0.20606610115538251, 0.20612912609222087, 0.20619222276071283,
    0.20625540439752824, 0.20631863874323825, 0.2063246859990536, 0.20633073713868222,
    0.2063368115019291, 0.2063428981507078, 0.20634898551283998, 0.20635509411705302,
    0.2063612051783724, 0.2063673423959854, 0.20637346471243717, 0.20637960874653877,
    0.20638013060422652, 0.20638065341305226, 0.20638119564094443, 0.20638173438133078,
    0.20638226701858195, 0.2063827967231578, 0.2063833402984954, 0.20638388197155708,
    0.20638440731675067, 0.20638494011252484, 0.2063849480386744, 0.20638496531768047,
    0.20638497498758296, 0.20638499012652864, 0.20638501025894854
]

# Plotting
plt.figure(figsize=(10, 5))
plt.plot(epochs, train_losses, label='Training Loss')
plt.plot(epochs, val_losses, label='Validation Loss')
plt.title('Training and Validation Losses over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()