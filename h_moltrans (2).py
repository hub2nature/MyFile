# -*- coding: utf-8 -*-
"""H- MolTrans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12WsUOZStIExf-l1dleAbNDzmmVd9aAc5
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/DTI_prediction/MolTrans/

# Commented out IPython magic to ensure Python compatibility.
# %cd MolTrans

!pip install torch

!pip install pandas

import os
import torch
import random
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

def set_seed(seed_value=42):
        os.environ['PYTHONHASHSEED'] = str(seed_value)
        random.seed(seed_value)
        np.random.seed(seed_value)
        torch.manual_seed(seed_value)
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False




set_seed(42)

"""# **DAVIS data on GCVAE**"""

!pip install torch_geometric

!pip install rdkit

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, global_mean_pool, GraphConv
from torch_geometric.nn import TransformerConv
from rdkit import Chem
from rdkit.Chem import rdmolops
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt

# Load and preprocess datasets
file_path = '/mnt/data/MolTrans.csv'
df = pd.read_csv(file_path)
df = df.sample(n=1000, random_state=42).reset_index(drop=True)
df = df[['SMILES', 'Target Sequence', 'Label']]

df_val = pd.read_csv('/mnt/data/MolTrans_val.csv')
df_val = df_val.sample(n=10, random_state=42).reset_index(drop=True)
df_val = df_val[['SMILES', 'Target Sequence', 'Label']]

df_test = pd.read_csv('/mnt/data/MolTrans_test.csv')
df_test = df_test.sample(n=10, random_state=42).reset_index(drop=True)
df_test = df_test[['SMILES', 'Target Sequence', 'Label']]

# Function to convert SMILES to graph
def smiles_to_graph(smiles, max_atoms=128):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None, None
    adj = rdmolops.GetAdjacencyMatrix(mol)
    features = []
    for atom in mol.GetAtoms():
        features.append([atom.GetAtomicNum()])
    x = torch.tensor(features, dtype=torch.long)
    adj = torch.tensor(adj, dtype=torch.long)

    # Padding
    if x.shape[0] < max_atoms:
        pad_x = torch.zeros((max_atoms - x.shape[0], x.shape[1]), dtype=torch.long)
        x = torch.cat((x, pad_x), dim=0)
        pad_adj = torch.zeros((max_atoms - adj.shape[0], adj.shape[1]), dtype=torch.long)
        adj = torch.cat((adj, pad_adj), dim=0)
        pad_adj = torch.zeros((adj.shape[0], max_atoms - adj.shape[1]), dtype=torch.long)
        adj = torch.cat((adj, pad_adj), dim=1)
    else:
        x = x[:max_atoms]
        adj = adj[:max_atoms, :max_atoms]

    edge_index = adj.nonzero(as_tuple=False).t().contiguous()
    return x, edge_index

# Function to convert protein sequences to tokenized sequences
def protein_to_token(sequence, max_length=1000):
    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
    aa_index = {aa: i for i, aa in enumerate(amino_acids)}
    tokenized_seq = [aa_index[aa] for aa in sequence if aa in aa_index]
    tokenized_seq = tokenized_seq[:max_length] + [0] * (max_length - len(tokenized_seq))
    return torch.tensor(tokenized_seq, dtype=torch.long)

# Convert datasets
def create_smiles_graph_data(df):
    data_list = []
    for i, row in df.iterrows():
        x, edge_index = smiles_to_graph(row['SMILES'])
        if x is not None:
            data = Data(x=x, edge_index=edge_index, y=torch.tensor([row['Label']], dtype=torch.float))
            data_list.append(data)
    return data_list

def create_protein_token_data(df, max_length=1000):
    data_list = []
    for i, row in df.iterrows():
        tokenized_seq = protein_to_token(row['Target Sequence'], max_length)
        data = Data(x=tokenized_seq, y=torch.tensor([row['Label']], dtype=torch.float))
        data_list.append(data)
    return data_list

train_smiles_data = create_smiles_graph_data(df)
val_smiles_data = create_smiles_graph_data(df_val)
test_smiles_data = create_smiles_graph_data(df_test)

train_protein_data = create_protein_token_data(df)
val_protein_data = create_protein_token_data(df_val)
test_protein_data = create_protein_token_data(df_test)

train_smiles_loader = DataLoader(train_smiles_data, batch_size=32, shuffle=True)
val_smiles_loader = DataLoader(val_smiles_data, batch_size=32, shuffle=False)
test_smiles_loader = DataLoader(test_smiles_data, batch_size=32, shuffle=False)

train_protein_loader = DataLoader(train_protein_data, batch_size=32, shuffle=True)
val_protein_loader = DataLoader(val_protein_data, batch_size=32, shuffle=False)
test_protein_loader = DataLoader(test_protein_data, batch_size=32, shuffle=False)

# Define the hierarchical graph transformer for SMILES
class HierarchicalGraphTransformer(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, num_layers):
        super(HierarchicalGraphTransformer, self).__init__()
        self.embedding = nn.Linear(in_channels, hidden_channels)
        self.layers = nn.ModuleList([
            TransformerConv(hidden_channels, hidden_channels // num_heads, heads=num_heads, dropout=0.1)
            for _ in range(num_layers)
        ])
        self.pool = nn.Linear(hidden_channels, out_channels)
        self.final_conv = TransformerConv(out_channels, out_channels // num_heads, heads=num_heads, dropout=0.1)

    def forward(self, x, edge_index, batch):
        x = self.embedding(x.float())
        for layer in self.layers:
            x = layer(x, edge_index)
            x = F.relu(x)
        x = global_mean_pool(x, batch)
        x = self.pool(x)
        x = self.final_conv(x, edge_index)
        return x

# Define the transformer for protein sequences
class ProteinTransformer(nn.Module):
    def __init__(self, num_tokens, embed_dim, num_heads, num_encoder_layers, dropout=0.1):
        super(ProteinTransformer, self).__init__()
        self.embedding = nn.Embedding(num_tokens, embed_dim)
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True),
            num_layers=num_encoder_layers
        )
        self.fc = nn.Linear(embed_dim, embed_dim)

    def forward(self, proteins):
        proteins_emb = self.embedding(proteins)  # Transformer with batch_first=True expects input as (batch_size, seq_len, embed_dim)
        proteins_encoded = self.encoder(proteins_emb).mean(dim=1)  # Global average pooling
        return self.fc(proteins_encoded)

# Define the combined DTI prediction model
class DTIModel(nn.Module):
    def __init__(self, smiles_transformer, protein_transformer, hidden_dim):
        super(DTIModel, self).__init__()
        self.smiles_transformer = smiles_transformer
        self.protein_transformer = protein_transformer
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)

    def forward(self, smiles_data, protein_data):
        smiles_out = self.smiles_transformer(smiles_data.x, smiles_data.edge_index, smiles_data.batch)
        protein_out = self.protein_transformer(protein_data.x)
        combined = torch.cat((smiles_out, protein_out), dim=1)
        x = F.relu(self.fc1(combined))
        x = self.fc2(x)
        return x

# Model Initialization
hidden_dim = 64
num_heads = 4
num_layers = 3
num_tokens = 21  # 20 amino acids + 1 padding token

smiles_transformer = HierarchicalGraphTransformer(in_channels=1, hidden_channels=hidden_dim, out_channels=hidden_dim, num_heads=num_heads, num_layers=num_layers)
protein_transformer = ProteinTransformer(num_tokens=num_tokens, embed_dim=hidden_dim, num_heads=num_heads, num_encoder_layers=num_layers, dropout=0.1)
dti_model = DTIModel(smiles_transformer, protein_transformer, hidden_dim)

# Training and evaluation functions
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dti_model = dti_model.to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(dti_model.parameters(), lr=0.001)

def train(model, smiles_loader, protein_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for smiles_data, protein_data in zip(smiles_loader, protein_loader):
        optimizer.zero_grad()
        smiles_data = smiles_data.to(device)
        protein_data = protein_data.to(device)
        output = model(smiles_data, protein_data)
        loss = criterion(output, smiles_data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(smiles_loader)

def evaluate(model, smiles_loader, protein_loader, criterion, device):
    model.eval()
    total_loss = 0
    all_labels = []
    all_preds = []
    with torch.no_grad():
        for smiles_data, protein_data in zip(smiles_loader, protein_loader):
            smiles_data = smiles_data.to(device)
            protein_data = protein_data.to(device)
            output = model(smiles_data, protein_data)
            loss = criterion(output, smiles_data.y)
            total_loss += loss.item()
            all_labels.append(smiles_data.y.cpu())
            all_preds.append(output.cpu())
    all_labels = torch.cat(all_labels)
    all_preds = torch.cat(all_preds)
    auc = roc_auc_score(all_labels, all_preds)
    return total_loss / len(smiles_loader), auc

# Training loop
num_epochs = 50
train_losses = []
val_losses = []
val_aucs = []

for epoch in range(num_epochs):
    train_loss = train(dti_model, train_smiles_loader, train_protein_loader, optimizer, criterion, device)
    val_loss, val_auc = evaluate(dti_model, val_smiles_loader, val_protein_loader, criterion, device)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_aucs.append(val_auc)
    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}')

# Plotting the training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Test evaluation
test_loss, test_auc = evaluate(dti_model, test_smiles_loader, test_protein_loader, criterion, device)
print(f'Test Loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}')